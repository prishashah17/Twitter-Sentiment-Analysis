{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65610db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/prishashah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110ed49",
   "metadata": {},
   "source": [
    "<b>Preparing Training Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd9683",
   "metadata": {},
   "source": [
    "<b>Importing Training Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f9cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv')\n",
    "df_train = df_train[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59311ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adf8af",
   "metadata": {},
   "source": [
    "<b> Splitting the Tweet text into words using NLTK </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6886b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = []\n",
    "for i in range(len(training_data)):\n",
    "    tweets_train.append([word_tokenize(training_data[i][0]), training_data[i][1]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b7c96",
   "metadata": {},
   "source": [
    "<b>Cleaning the Words using WordNetLemmatizer available in NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066ebfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0799d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5efa534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_tweets(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.isalpha():\n",
    "            if w.lower() not in stops:\n",
    "                pos = pos_tag([w])\n",
    "                clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "                output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6d2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets_train)):\n",
    "    tweets_train[i] = (clean_tweets(tweets_train[i][0]), tweets_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22ec25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "tweets = []\n",
    "for tweet, sentiment in tweets_train:\n",
    "    tweets.append(\" \".join(tweet))\n",
    "    y_train.append(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b11c6a",
   "metadata": {},
   "source": [
    "<b>Using Count Vectorizer to get the X Train</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e35a104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=2000) # Tried using n grams but the accuracy was decreasing\n",
    "x_train_features = count_vec.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01865c",
   "metadata": {},
   "source": [
    "<b>Preparing Testing Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b76b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "testing_data = np.array(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f7cf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = []\n",
    "for t in testing_data:\n",
    "    t = clean_tweets(word_tokenize(t))\n",
    "    tweets_test.append(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc19252",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_features = count_vec.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c9669",
   "metadata": {},
   "source": [
    "<b>Performing Classification</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebafe07",
   "metadata": {},
   "source": [
    "<b>Support Vector Machine</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6808f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8c735a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svc.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b875127",
   "metadata": {},
   "source": [
    "<b>Random Forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad9d3cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895455a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred_rf)\n",
    "df.to_csv('predictions_rf.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ab690",
   "metadata": {},
   "source": [
    "<b>Multinomial Naive Bayes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20b15422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnv = MultinomialNB(alpha = 1)\n",
    "mnv.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1b80f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mnv = mnv.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a0973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred_mnv)\n",
    "df.to_csv('predictions_mnv.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0596f",
   "metadata": {},
   "source": [
    "<b>Descision Tree</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "218d9501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b78f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9ada2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred_dt)\n",
    "df.to_csv('predictions_dt.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a117fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
